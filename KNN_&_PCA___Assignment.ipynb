{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFvgxQbcWruF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **KNN & PCA | Assignment**"
      ],
      "metadata": {
        "id": "E3wKEhLPWuxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "- Answer:\n",
        "K-Nearest Neighbors (KNN) is a simple and intuitive machine learning algorithm that works on the idea of similarity. It does not learn a model during training. Instead, it stores all the training data and makes predictions only when a new data point is given.\n",
        "\n",
        "- In classification, KNN looks at the K nearest data points to the new sample and assigns the class that appears most frequently among those neighbors. For example, if most nearby points belong to class A, the new point is classified as class A.\n",
        "\n",
        "- regression, KNN predicts a numerical value by taking the average of the values of the K nearest neighbors. The idea is that similar data points usually have similar output values.\n",
        "\n",
        "**Question 2:** What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "Answer:\n",
        "- The Curse of Dimensionality refers to problems that occur when working with data having a large number of features. As dimensions increase, the distance between data points becomes less meaningful.\n",
        "\n",
        "- Since KNN depends heavily on distance calculations, high-dimensional data makes it difficult to correctly identify nearest neighbors. All points start to look equally distant, which reduces the accuracy of KNN and increases computational cost.\n",
        "\n",
        "- As a result, KNN performs poorly on high-dimensional datasets unless dimensionality reduction techniques like PCA are applied.\n",
        "\n",
        "**Question 3:** What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Answer:\n",
        "- Principal Component Analysis (PCA) is a dimensionality reduction technique used to reduce the number of features while preserving most of the dataâ€™s information. It does this by transforming original features into new variables called principal components.\n",
        "\n",
        "- Feature selection chooses a subset of existing features, whereas PCA creates new features by combining original ones. PCA focuses on maximizing variance, while feature selection focuses on choosing the most relevant features directly.\n",
        "\n",
        "**Question 4**: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "Answer:\n",
        "- In PCA, eigenvectors represent the directions in which data varies the most, and eigenvalues represent how much variance exists along those directions.\n",
        "\n",
        "- Eigenvectors decide the direction of principal components, while eigenvalues help decide which components are important. Components with higher eigenvalues carry more information and are selected first in PCA.\n",
        "\n",
        "**Question 5:** How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "Answer:\n",
        "- KNN works best when distances between data points are meaningful. PCA helps by reducing noise and removing less important dimensions, making distance calculations more reliable.\n",
        "\n",
        "- Using PCA before KNN improves accuracy, reduces overfitting, and speeds up computation. Together, they create an efficient and well-balanced machine learning pipeline."
      ],
      "metadata": {
        "id": "wTiDQvwDW0ij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6:Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "\n",
        "Answer : The first few components explain most of the variance, meaning dimensionality can be reduced without losing much information."
      ],
      "metadata": {
        "id": "ng6rgFPvXhcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"Accuracy without scaling:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn.predict(X_test_scaled)\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, y_pred_scaled))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKLc0xwuXgwB",
        "outputId": "eb69ddd7-f277-4e5b-cf00-1bd33469038b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n",
            "Accuracy with scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "\n",
        "Answer:\n",
        "Observation:\n",
        "- Accuracy is slightly lower than the full dataset but still competitive, showing PCA effectively reduces dimensions while retaining useful information."
      ],
      "metadata": {
        "id": "75yyx4f9YFsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pca, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26-esYVfYQDX",
        "outputId": "3283ecdc-683f-4366-8da6-0ae4409d7296"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "Answer:\n",
        "- Both metrics perform well, but Euclidean distance slightly outperforms Manhattan distance on this dataset."
      ],
      "metadata": {
        "id": "D544ctDQYsH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Euclidean Accuracy:\", accuracy_score(y_test, knn_euclidean.predict(X_test_scaled)))\n",
        "print(\"Manhattan Accuracy:\", accuracy_score(y_test, knn_manhattan.predict(X_test_scaled)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElnGyxfyY0rv",
        "outputId": "c04091e9-132c-4e52-f409-e2dcf5a56bf7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Accuracy: 0.9444444444444444\n",
            "Manhattan Accuracy: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** Explain a PCA + KNN pipeline for high-dimensional gene expression data.\n",
        "\n",
        "Answer:\n",
        "- For high-dimensional gene expression data, PCA is first applied to reduce the number of features while retaining maximum variance. The number of components is chosen based on explained variance (for example, 95%).\n",
        "\n",
        "- After dimensionality reduction, KNN is used for classification since the reduced dataset improves distance calculations and reduces overfitting.\n",
        "\n",
        "- Model evaluation is done using accuracy, precision, recall, and cross-validation. This pipeline is justified to stakeholders as it improves performance, reduces noise, and provides reliable results for real-world biomedical applications."
      ],
      "metadata": {
        "id": "zaF_kQ8AY38-"
      }
    }
  ]
}